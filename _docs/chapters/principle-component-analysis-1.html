
<div id="principle-component-analysis-of-a-binary-classification-system" class="section level1">
<h1><span class="header-section-number">4</span> Principle Component Analysis of A Binary Classification System</h1>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>This chapter describes the use and functional understanding of Principle Component Analysis (PCA). PCA is very popular and commonly used during the early phases of model development to provide information on variance. In particular, PCA is a transformative process that orders and maximizes variances found within a dataset.</p>
<blockquote>
<p>The primary goal of principal components analysis is to reveal the hidden structure in a dataset. In so doing, we may be able to; <a href="#fn43" class="footnote-ref" id="fnref43"><sup>43</sup></a></p>
<ol style="list-style-type: decimal">
<li><p>identify how different variables work together to create the dynamics of the system,</p></li>
<li><p>reduce the dimensionality of the data,</p></li>
<li><p>decrease redundancy in the data,</p></li>
<li><p>filter some of the noise in the data,</p></li>
<li><p>compress the data,</p></li>
<li><p>prepare the data for further analysis using other techniques.</p></li>
</ol>
</blockquote>
<div id="advantages-of-using-pca-include" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Advantages Of Using PCA Include</h3>
<ol style="list-style-type: decimal">
<li><p>PCA preserves the global structure among the data points,</p></li>
<li><p>It is efficiently applied to large data sets,</p></li>
<li><p>PCA may provide information on the importance of features found in the original datasets.</p></li>
</ol>
</div>
<div id="disadvantages-of-pca-should-be-considered" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Disadvantages Of PCA Should Be Considered</h3>
<ol style="list-style-type: decimal">
<li><p>PCA can easily suffer from scale complications,</p></li>
<li><p>Similarly to the point above, PCA is susceptible to significant outliers. If the number of samples is small or when values have many potential outliers, this can influence scaling and relative point placement,</p></li>
<li><p>Intuitive understanding can be tricky.</p></li>
</ol>
</div>
<div id="data-centering-scaling-normalization" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Data centering / scaling / normalization</h3>
<p>It is common for the first step when carrying out a PCA is to center, scale, normalize the data. This is important due the fact that PCA is sensitive to the scale of the features. If the features are quite different frome each other, i.e. different by one or more orders of magnitude then scaling is crucial.</p>
<p>While determining the variance of your dataset, it should be clear that the order of magnitude of your data features matters significantly. The reasons for this should be clear that if one axis is in 1,000’s while the second axis is between 1 and 10, the larger scale will have a higher variance distorting the results.</p>
<p>What do the center and scale arguments do in the <code>prcomp</code> command?</p>
<p>There are four common methods for scaling data:</p>
<table>
<colgroup>
<col width="22%" />
<col width="77%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Method</th>
<th align="center">Formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Centering</td>
<td align="center"><span class="math inline">\(f(x) ~=~ \large x - \bar x\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Scaling [0, 1]</td>
<td align="center"><span class="math inline">\(f(x) ~=~ \Large \frac {x - min(x)} {max(x) - min(x)}\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Scaling [a, b]</td>
<td align="center"><span class="math inline">\(f(x) ~=~ \large (b - a)* \Large \frac {x - min(x)} {max(x) - min(x)} + a\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">Normalizing</td>
<td align="center"><span class="math inline">\(f(x) ~=~ \Large \frac {x - mean(x)} {\sigma_x}\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="histograms-of-scaled-vs.-unscaled-data" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Histograms of Scaled Vs. Unscaled data</h3>
<p>Investigating the differences between the amino acid Phenylalanine (F) before and after 2 scaling methods.
<img src="_main_files/figure-html/32-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Investigating the plots above, the main idea to recognize is that the data has not been fundamentally changed, simply ‘shifted and stretched’ or more accurately transformed. It appears that any visible changes of the distributions can be accounted for by differing binnings.</p>
<p>Although the differences are between all three histograms are minor, any transformation <em>would</em> be sufficient to use. However, I chose to use the <em>Normalized</em> dataset.</p>
</div>
<div id="finding-the-covariance-matrix" class="section level3">
<h3><span class="header-section-number">4.1.5</span> Finding the Covariance Matrix</h3>
<p>The first step for calculating PCA is to determine the Covariance matrix. Covariance provides a measure of how strongly variables change together.<a href="#fn44" class="footnote-ref" id="fnref44"><sup>44</sup></a> <a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a></p>
</div>
<div id="covariance-of-two-variables" class="section level3 unnumbered">
<h3>Covariance of two variables</h3>
<p>Remember, this simplified formula is to determine covariance for a two-dimensional system.</p>
<p><span class="math display">\[\begin{equation}
cov(x, y) ~=~ \frac{1}{N} \sum_{i=1}^N (x_i - \bar x) (y_i - \bar y)
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(N\)</span> is the number of observations, <span class="math inline">\(\bar x\)</span> is the mean of the independent variable, <span class="math inline">\(\bar y\)</span> is the mean of the dependent variable.</p>
</div>
<div id="covariance-of-matrices" class="section level3 unnumbered">
<h3>Covariance of matrices</h3>
<p>When dealing with a many feature variables one needs to determine the covariance of matrices, <span class="math inline">\(\large M\)</span> using linear algebra. <a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a></p>
<ol style="list-style-type: decimal">
<li>Find the column means of the matrix, <span class="math inline">\(M_{means}\)</span>.</li>
<li>Find the difference matrix, <span class="math inline">\(D ~=~ M - M_{means}\)</span>.</li>
<li>Finally calculate the covariance matrix:</li>
</ol>
<p><span class="math display">\[\begin{equation}
cov ~ (M) ~=~ \left( \frac{1}{N-1} \right) ~ D^T \cdot D, ~~~ where ~~~~~ D = M - M_{means}
\end{equation}\]</span></p>
<p>Where <span class="math inline">\(D^T\)</span> is the transpose of the difference matrix, <span class="math inline">\(N\)</span> is the number of observations or rows in this case.</p>
</div>
<div id="finding-pca-via-singular-value-decomposition" class="section level3">
<h3><span class="header-section-number">4.1.6</span> Finding PCA via singular value decomposition</h3>
<p>The procedure below is an outline, not the full computation of PCA.</p>
<p>This procedure for PCA relies on the fact that it is similar to the singular value decomposition (SVD) used when determining eigenvectors and eigenvalues. <a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a></p>
<blockquote>
<p>Singular value decomposition says that every n x p matrix can be written as the product of three matrices: <span class="math inline">\(A = U \Sigma V^T\)</span> where:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(U\)</span> is an orthogonal n x n matrix.</li>
<li><span class="math inline">\(\Sigma\)</span> is a diagonal n x p matrix. In practice, the diagonal elements are ordered so that <span class="math inline">\(\Sigma_{ii} ~\geqq~ \Sigma_{jj}\)</span> for all i &lt; j.</li>
<li><span class="math inline">\(V\)</span> is an orthogonal p x p matrix, and <span class="math inline">\(V^T\)</span> represents a matrix transpose.</li>
</ol>
<p>The SVD represents the essential geometry of a linear transformation. It tells us that every linear transformation is a composition of three fundamental actions. Reading the equation from right to left:</p>
<ol style="list-style-type: decimal">
<li>The matrix <span class="math inline">\(V\)</span> represents a rotation or reflection of vectors in the p-dimensional domain.</li>
<li>The matrix <span class="math inline">\(\Sigma\)</span> represents a linear dilation or contraction along each of the p coordinate directions. If n <span class="math inline">\(\neq\)</span> p, this step also canonically embeds (or projects) the p-dimensional domain into (or onto) the n-dimensional range.</li>
<li>The matrix <span class="math inline">\(U\)</span> represents a rotation or reflection of vectors in the n-dimensional range.</li>
</ol>
</blockquote>
<p>The intuition for understanding PCA is reasonably straightforward. Consider the 2-dimensional data cloud of points or observations in a hypothetical experiment, as seen in the figure on the left. Variances along both the x and y dimensions are calculated. However, given the data shown, there is a rotation of that x-y plane, which will present the data showing its most significant variance. This variance will reside on an axis analogous to points on an Ordinary Least Squares (OLS) line. This axis is called the <em>first principle component</em> followed by the second principal component and so on.</p>
<p>Unlike an OLS calculation, PCA will determine not only the first and most significant variance of your data set, but it will, through the rotation and transform your dataset via linear algebra, calculating N variances within your dataset, where N is equal to the number of features in the dataset. The second principal component will be calculated only along a coordinate axis, which is perpendicular (orthogonal or orthonormal) to the first. Each subsequent principal component will then be calculated along axes which are orthogonal to each other. A further benefit of using PCA is that the variances it reports will be ranked in order from highest to lowest. <a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a></p>
</div>
<div id="example-of-two-dimensional-pca-using-random-data" class="section level3">
<h3><span class="header-section-number">4.1.7</span> Example of two-dimensional PCA using random data</h3>
<p><img src="_main_files/figure-html/34-1.png" width="672" /></p>
<table>
<thead>
<tr class="header">
<th align="left">Graphic</th>
<th align="center">Range (Green lines)</th>
<th align="center">Differences</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Raw Data (Left)</td>
<td align="center">1 &lt;= x &lt;= 4</td>
<td align="center">3 units</td>
</tr>
<tr class="even">
<td align="left">Transformed Data (Right)</td>
<td align="center">-2.45 &lt;= x &lt;= 2.77</td>
<td align="center">5.22 units</td>
</tr>
</tbody>
</table>
<p>If we investigate the figures above we find that the range of the samples is (1 &lt;= x &lt;= 4),
while the range for the transformed data is (-2.45 &lt;= x &lt;= 2.76). The differences between the two ranges are 3 and 5.21 units, respectively. The rotation should be no surprise since the PCA is essentially a maximization of variance.</p>
<p>Many R-packages will carry out the steps for PCA all behind the ‘scenes’ but giving no greater understanding for beginners. For example, <code>stats::prcomp</code> <a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a>, <code>stats::princomp</code> <a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a> are most commonly used. However, there are dozens of similar packages. A keyword search for <em>PCA</em> at R-cran <a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a> provides 78 matches, as of November 2019.</p>
</div>
</div>
<div id="principle-component-analysis-using-norm_c_m_20aa" class="section level2">
<h2><span class="header-section-number">4.2</span> Principle component analysis using <code>norm_c_m_20aa</code></h2>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1">start_time &lt;-<span class="st"> </span><span class="kw">Sys.time</span>() <span class="co"># Start timer</span></a>
<a class="sourceLine" id="cb57-2" data-line-number="2"></a>
<a class="sourceLine" id="cb57-3" data-line-number="3">c_m_<span class="dv">20</span>_PCA &lt;-<span class="st"> </span><span class="kw">prcomp</span>(norm_c_m_20aa)</a>
<a class="sourceLine" id="cb57-4" data-line-number="4"></a>
<a class="sourceLine" id="cb57-5" data-line-number="5"><span class="kw">Sys.time</span>() <span class="op">-</span><span class="st"> </span>start_time <span class="co"># End timer &amp; display time difference</span></a></code></pre></div>
<pre><code>## Time difference of 0.02533388 secs</code></pre>
<div id="screeplot-cumulative-proportion-of-variance-plot" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Screeplot &amp; Cumulative Proportion of Variance plot</h3>
<p>Two plots are commonly used to determine the number of principal components that a researcher would generally accept as useful. The eigenvalues derived from PCA are proportional to the variances which they represent, and depending on the strategy used to calculate them, the eigenvalues are equal to the variances of the components.</p>
<p>The first of the two plots which I which is the scree plot. <a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a> The scree plot is a ranked list of the eigenvalues plotted against its principal components. An eigenvalue score of one is thought to provide a comparable amount of information as a single variable un-transformed by PCA.</p>
<p>The second plot describes the cumulative proportion of variance versus the principal component. This graphic shows how much each principal component represents the entire cumulative variances or total squared error.</p>
<p><span class="math display">\[\begin{equation}
Cumlative ~ Proportion ~of ~Variance ~=~ \frac{\sigma_i^2}{\sum_{i=1}^N \sigma_i^2}
\end{equation}\]</span></p>
<p>Here again, there are several criteria regarding how best to use the information from the is plot. The first of which is Cattell’s heuristic. Cattell advises using the principal component that is above the elbow of the curve. The second heuristic is keeping the total number of factors that best explains 80%-95% of the variance. There is no hard-fast rule at this time; a set of researchers only uses the first three factors or none at all.<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a> A second suggestion is to use the Kaiser rule, which states it is sufficient to use Principal Components, which have an eigenvalue greater than or equal to one. <a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a></p>
<p><img src="_main_files/figure-html/36-1.png" width="672" style="display: block; margin: auto;" /></p>
<hr />
<p><img src="_main_files/figure-html/37-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>If we investigate the ‘cumulative proportion of variance’ plot, we see an arbitrary line on the Y-axis, which denotes the 90% mark. At this point, the plot suggests that a researcher could use the most significant 12 of the variances from the PCA.</p>
</div>
<div id="biplots" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Biplots</h3>
<div id="biplot-1-pc1-vs.-pc2-with-class-by-color-labels" class="section level4">
<h4><span class="header-section-number">4.2.2.1</span> Biplot 1: PC1 Vs. PC2 with ‘Class’ by color labels</h4>
<ul>
<li><p>Black indicates control protein set, Class = 0</p></li>
<li><p>Blue indicates myoglobin protein set, Class = 1</p></li>
</ul>
<p>The first two principal components describe 46.95% of the variance.</p>
</div>
<div id="biplot-2-determination-of-4-rule-set-for-outliers" class="section level4">
<h4><span class="header-section-number">4.2.2.2</span> Biplot 2: Determination Of 4 Rule Set For Outliers</h4>
<p><img src="_main_files/figure-html/310-1.png" width="576" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="obtain-anomalous-points-from-biplot-2-pc1-vs.-pc2" class="section level3">
<h3><span class="header-section-number">4.2.3</span> Obtain Anomalous Points From Biplot #2: PC1 Vs. PC2</h3>
<p>Anomalous data points are data that is greater than the absolute value of 3 sigma, Anomalous Point &gt; <span class="math inline">\(| 3 \sigma |\)</span>.</p>
<p>I have chosen to analyze the PCA biplot of the first and second principal components. The first and second components were used because they describe nearly 50% of the variance (46.95%).</p>
</div>
<div id="outliers-from-principal-component-1" class="section level3">
<h3><span class="header-section-number">4.2.4</span> Outliers from Principal Component-1</h3>
<p>Rule Set Given PC1:</p>
<ol style="list-style-type: decimal">
<li><p>Outlier_1: c_m_20_PCA$x[, 1] &gt; 3 standard deviations</p></li>
<li><p>Outlier_2: c_m_20_PCA$x[, 1] &lt; -3 standard deviations</p></li>
</ol>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1">outliers_PC1 &lt;-<span class="st"> </span><span class="kw">which</span>((c_m_<span class="dv">20</span>_PCA<span class="op">$</span>x[, <span class="dv">1</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span>) <span class="op">|</span><span class="st"> </span>(c_m_<span class="dv">20</span>_PCA<span class="op">$</span>x[, <span class="dv">1</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">-3</span>))</a>
<a class="sourceLine" id="cb59-2" data-line-number="2"><span class="kw">length</span>(outliers_PC1)</a></code></pre></div>
<pre><code>## [1] 285</code></pre>
</div>
<div id="outliers-from-principal-component-2" class="section level3">
<h3><span class="header-section-number">4.2.5</span> Outliers from Principal Component-2</h3>
<p>Rule Set Given PC2:</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Outlier_3: c_m_20_PCA$x[, 2] &gt; 3 standard deviations</p></li>
<li><p>Outlier_4: c_m_20_PCA$x[, 2] &lt; -3 standard deviations</p></li>
</ol>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1">outliers_PC2 &lt;-<span class="st"> </span><span class="kw">which</span>((c_m_<span class="dv">20</span>_PCA<span class="op">$</span>x[, <span class="dv">2</span>] <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span>) <span class="op">|</span><span class="st"> </span>(c_m_<span class="dv">20</span>_PCA<span class="op">$</span>x[, <span class="dv">2</span>] <span class="op">&lt;</span><span class="st"> </span><span class="dv">-3</span>))</a>
<a class="sourceLine" id="cb61-2" data-line-number="2"><span class="kw">length</span>(outliers_PC2)</a></code></pre></div>
<pre><code>## [1] 177</code></pre>
</div>
<div id="list-of-all-outliers-union-and-sorted-found-using-the-ruleset-1-through-4" class="section level3">
<h3><span class="header-section-number">4.2.6</span> List of all outliers (union and sorted) found using the ruleset 1 through 4</h3>
<ul>
<li>The list of total outliers is derived by taking the <code>union</code> of <code>outliers_PC1</code> and <code>outliers_PC2</code> and then using <code>sort.</code></li>
</ul>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1">total_pca_<span class="dv">1</span>_<span class="dv">2</span>_outliers &lt;-<span class="st"> </span><span class="kw">union</span>(outliers_PC1, outliers_PC2)</a>
<a class="sourceLine" id="cb63-2" data-line-number="2">total_pca_<span class="dv">1</span>_<span class="dv">2</span>_outliers &lt;-<span class="st"> </span><span class="kw">sort</span>(total_pca_<span class="dv">1</span>_<span class="dv">2</span>_outliers)</a>
<a class="sourceLine" id="cb63-3" data-line-number="3"></a>
<a class="sourceLine" id="cb63-4" data-line-number="4"><span class="kw">length</span>(total_pca_<span class="dv">1</span>_<span class="dv">2</span>_outliers)</a></code></pre></div>
<pre><code>## [1] 461</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="co"># Write out to Outliers folder</span></a>
<a class="sourceLine" id="cb65-2" data-line-number="2"><span class="kw">write.table</span>(total_pca_<span class="dv">1</span>_<span class="dv">2</span>_outliers,</a>
<a class="sourceLine" id="cb65-3" data-line-number="3"><span class="dt">file =</span> <span class="st">&quot;./00-data/03-ml_results/pca_outliers.csv&quot;</span>,</a>
<a class="sourceLine" id="cb65-4" data-line-number="4"><span class="dt">row.names =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb65-5" data-line-number="5"><span class="dt">na =</span> <span class="st">&quot;&quot;</span>,</a>
<a class="sourceLine" id="cb65-6" data-line-number="6"><span class="dt">col.names =</span> <span class="st">&quot;rowNum&quot;</span>,</a>
<a class="sourceLine" id="cb65-7" data-line-number="7"><span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)</a></code></pre></div>
<p>It is important to remember and understand that this list of “total_pca_1_2_outliers” includes BOTH negative and positive controls. The groupings are as follows:</p>
<table>
<thead>
<tr class="header">
<th align="left">Group</th>
<th align="right">Range of Groups</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Controls</td>
<td align="right">1, …, 1216</td>
</tr>
<tr class="even">
<td align="left">Positive (Myoglobin)</td>
<td align="right">1217, …, 2341</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="pca-conclusion" class="section level2">
<h2><span class="header-section-number">4.3</span> PCA Conclusion</h2>
<p>Principal Component Analysis is very popular and an excellent choice to include during Exploratory Data. Analysis. One objective for using PCA is to filter noise from the dataset used and, in turn, increase any signal or to sufficiently delineate observations from each other. In fact, in the figure below, there are five colored groups outside the main body of observations that are marked at ‘outliers.’ The number of outliers obtained from PCA is 461 proteins. The premise of this experiment is to determine if PCA is an excellent representative measure for proteins that are categorized is false-positive, and false-negatives in the five subsequent machine learning model approach. It will be interesting to see if anyone of these groups will be present in the group of false-positives and false-negatives in any of the machine learning models.</p>
<div id="outliers-derived-from-pc1-vs-pc2" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Outliers derived from PC1 Vs PC2</h3>
<p>The table and the figure below show a subset of outliers produced when the first and second principal component is graphed. My interest lies in finding if any one of the lettered groups (A-E) are part of the false-positives and false-negatives from each of the machine learning models. Each of the five groups is rich is a small number of amino acids. We hope that this information will shine a light on how the different machine models work. It is also expected that this will give help in constructing a model that is more interpretable for the more difficult opaque machine learning models, such as Random Forest, Neural Networks, and possibly Support Vector Machine using the Radial Basis Function.</p>
<table>
<thead>
<tr class="header">
<th align="center">Group</th>
<th align="center">Increased concentration of amino acid</th>
<th align="center">Example observations</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">A</td>
<td align="center">H, L, K</td>
<td align="center">1478</td>
</tr>
<tr class="even">
<td align="center">B</td>
<td align="center">E, K</td>
<td align="center">1934, 1870, 2100</td>
</tr>
<tr class="odd">
<td align="center">C</td>
<td align="center">V, I, F, Y</td>
<td align="center">182, 1752, 2156</td>
</tr>
<tr class="even">
<td align="center">D</td>
<td align="center">C, S</td>
<td align="center">1360, 2240</td>
</tr>
<tr class="odd">
<td align="center">E</td>
<td align="center">G, D, Q</td>
<td align="center">664, 2304</td>
</tr>
</tbody>
</table>
<p><img src="00-data/10-images/Biplot1.annotated.png" width="55%" style="display: block; margin: auto;" /></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="43">
<li id="fn43"><p>Emily Mankin, Principal Components Analysis: A How-To Manual for R, <a href="http://people.tamu.edu/~alawing/materials/ESSM689/pca.pdf" class="uri">http://people.tamu.edu/~alawing/materials/ESSM689/pca.pdf</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref43" class="footnote-back">↩</a></p></li>
<li id="fn44"><p><a href="http://mathworld.wolfram.com/Covariance.html" class="uri">http://mathworld.wolfram.com/Covariance.html</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref44" class="footnote-back">↩</a></p></li>
<li id="fn45"><p>Trevor Hastie, Robert Tibshirani, Jerome Friedman, ‘The Elements of Statistical Learning; Data Mining, Inference, and Prediction,’ Second Edition, Springer, <a href="DOI:10.1007/978-0-387-84858-7" class="uri">DOI:10.1007/978-0-387-84858-7</a>, 2009<a href="principle-component-analysis-of-a-binary-classification-system.html#fnref45" class="footnote-back">↩</a></p></li>
<li id="fn46"><p><a href="http://mathworld.wolfram.com/Covariance.html" class="uri">http://mathworld.wolfram.com/Covariance.html</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref46" class="footnote-back">↩</a></p></li>
<li id="fn47"><p><a href="https://blogs.sas.com/content/iml/2017/08/28/singular-value-decomposition-svd-sas.html" class="uri">https://blogs.sas.com/content/iml/2017/08/28/singular-value-decomposition-svd-sas.html</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref47" class="footnote-back">↩</a></p></li>
<li id="fn48"><p>Brian Everitt, Torsten Hothorn, An Introduction to Applied Multivariate Analysis with R, Springer, <a href="DOI:10.1007/978-1-4419-9650-3" class="uri">DOI:10.1007/978-1-4419-9650-3</a>, 2011<a href="principle-component-analysis-of-a-binary-classification-system.html#fnref48" class="footnote-back">↩</a></p></li>
<li id="fn49"><p><a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html" class="uri">https://stat.ethz.ch/R-manual/R-devel/library/stats/html/prcomp.html</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref49" class="footnote-back">↩</a></p></li>
<li id="fn50"><p><a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/princomp.html" class="uri">https://stat.ethz.ch/R-manual/R-devel/library/stats/html/princomp.html</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref50" class="footnote-back">↩</a></p></li>
<li id="fn51"><p><a href="https://cran.r-project.org/web/packages/available_packages_by_name.html" class="uri">https://cran.r-project.org/web/packages/available_packages_by_name.html</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref51" class="footnote-back">↩</a></p></li>
<li id="fn52"><p>Raymond Cattell, “The scree test for the number of factors.” Multivariate Behavioral Research. 1 (2): 245–76. <a href="DOI:10.1207/s15327906mbr0102_10" class="uri">DOI:10.1207/s15327906mbr0102_10</a>, 1966<a href="principle-component-analysis-of-a-binary-classification-system.html#fnref52" class="footnote-back">↩</a></p></li>
<li id="fn53"><p>Nicole Radzill, Ph.D., personal communication.<a href="principle-component-analysis-of-a-binary-classification-system.html#fnref53" class="footnote-back">↩</a></p></li>
<li id="fn54"><p><a href="https://stats.stackexchange.com/questions/253535/the-advantages-and-disadvantages-of-using-kaiser-rule-to-select-the-number-of-pr" class="uri">https://stats.stackexchange.com/questions/253535/the-advantages-and-disadvantages-of-using-kaiser-rule-to-select-the-number-of-pr</a><a href="principle-component-analysis-of-a-binary-classification-system.html#fnref54" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</html>



